{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hmsc for modeling of species communities\n",
        "\n",
        "For meaningful ecological analysis, Hmsc model requires a matrix of abundances $Y$, dimensioned $(n_y \\times n_s)$, $n_y$ the sampling units (study sites) and $n_s$ the species. The model estimates (regresses) the mean occurrence (abundance or prevalence of the species) against the environmental covariates matrix $X$. One key objective is to relate community-level response to environmental variations (called fixed effects in HMSC context), and the second principal goal is to assess the associative pattern in the residuals (called random effects).\n",
        "\n",
        "Furthermore, to reflect non-interchangability properties of many filed data and to improve the predictive performance of above simple non-spatial Hmsc model, a spatially explicit model is available for the random effects. It is done by incorporating an additional input with coordinates of the sampling units and making the assumption that a priori the distribution of random effects is not i.i.d., but can reflect the spatial correlation. Alike many other models, used in spatial statistics, this is done by assigning a Gaussian Process (GP) prior for the values of random effects.\n",
        "\n",
        "However, for large datasets, the full spatial models may become computationally infeasible, since it scales as $n_y^3$. Two alternative approaches are implemented to overcome challenge this in the Hmsc: nearest-neighbour gaussian process (NNGP) and gaussian predictive process (GPP). This facilitates advancement by efficient use of largescale high-resolution ecological datasets.\n",
        "\n",
        "Additionally, Hmsc is capable of analysis that integrates information like species traits $T$ and phylogenetic relationships $C$ for trait- and pylogeny-level analysis. These extra data adjust the model structure for the fixed effects part of HMSC."
      ],
      "metadata": {
        "id": "gCBqLcepfhGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone repo, install dependencies and load packages\n",
        "\n",
        "We start off with cloning the repository, install missing dependencies, and load packages."
      ],
      "metadata": {
        "id": "P2BjmFXQf3KH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U3ctBuo91Zu",
        "outputId": "a3caaf88-682b-47e6-a61d-48f001cb25aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hmsc-hpc'...\n",
            "remote: Enumerating objects: 707, done.\u001b[K\n",
            "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 707 (delta 121), reused 125 (delta 65), pack-reused 508\u001b[K\n",
            "Receiving objects: 100% (707/707), 248.36 KiB | 7.76 MiB/s, done.\n",
            "Resolving deltas: 100% (449/449), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aniskhan25/hmsc-hpc.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ujson pyreadr wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbT-KOt0-OPV",
        "outputId": "e951644a-495f-42b3-a01e-d5594d10fcaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ujson\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyreadr\n",
            "  Downloading pyreadr-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.9/440.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadr) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadr) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadr) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyreadr) (1.16.0)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=05a975bb2d739110c160abb883681b56a25ea4732633a5d7ad3b0cee5cba5305\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, ujson, pyreadr\n",
            "Successfully installed pyreadr-0.5.0 ujson-5.8.0 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += f\":{os.path.join(os.getcwd(), 'hmsc-hpc')}\" # set env variables\n",
        "\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFDGh4ypthtn",
        "outputId": "9a7c1593-1628-440c-e40e-fbd9f9a97d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/env/python:/content/hmsc-hpc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from platform import python_version\n",
        "\n",
        "print(\"Python Version: \", python_version())\n",
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"TF Probability Version: \", tfp.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKxKjDv9_1Da",
        "outputId": "bdddec81-c510-466e-e104-89f4b0e324ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version:  3.10.12\n",
            "TF Version:  2.14.0\n",
            "TF Probability Version:  0.22.0\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load input data and model structure\n",
        "\n",
        "For this demo notebook, we start with generating a parameter grid based on the aforementioned parameters: the model type (non-spatial and spatial), the number of species $n_s$ and sampling units $n_y$. This grid is used to load pre-saved input data and model structure. For now, the data is for single-chain sampling runs."
      ],
      "metadata": {
        "id": "uSXXihhzMPoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Any\n",
        "from itertools import product\n",
        "\n",
        "def grid_parameters(parameters: dict[str, Iterable[Any]]) -> Iterable[dict[str, Any]]:\n",
        "    for params in product(*parameters.values()):\n",
        "        yield dict(zip(parameters.keys(), params))\n",
        "\n",
        "parameters = {\n",
        "    'model_type': ['ns', 'fu', 'pg', 'nn', 'ph'] # [non-spatial, full spatial, predictive guassian, nearest neighbor, phylogeny]\n",
        "    , 'ns': [10, 20, 40, 80, 160, 320, 622] # number of species\n",
        "    , 'ny': [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25955, 51910, 103820, 207640] # sampling units (site loadings)\n",
        "    }\n",
        "\n",
        "nChains = 1\n",
        "\n",
        "models = {}\n",
        "for settings in grid_parameters(parameters):\n",
        "    key = f\"{parameters['model_type'].index(settings['model_type'])}{settings['model_type']}_ns{settings['ns']:03d}_ny{settings['ny']:05d}\"\n",
        "    models[key] = settings\n",
        "\n",
        "print(f\"Parameter grid size: {len(models)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8IuDVRzAyGZ",
        "outputId": "35b78f63-ff32-4a26-d553-8fa2e670f018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter grid size: 420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select an example model from the grid to run."
      ],
      "metadata": {
        "id": "eEaLl9mYLpXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = 'ns'\n",
        "ns = 40 # species\n",
        "ny = 100 # site loadings\n",
        "\n",
        "nChains = 1\n",
        "\n",
        "key = f\"{parameters['model_type'].index(model_type)}{model_type}_ns{ns:03d}_ny{ny:05d}\"\n",
        "current_model = models[key]\n",
        "\n",
        "print(f\"Model tested: {current_model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6v6GkQaDU4m",
        "outputId": "ba9a1f14-e346-4309-fbc3-4dfa19af4be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model tested: {'model_type': 'ns', 'ns': 40, 'ny': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input data and model structure is located on Allas and are downloaded to this notebook session."
      ],
      "metadata": {
        "id": "hqa3AhVpL87v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_filename  = f\"init_{key}_chain{nChains:02d}.rds\"\n",
        "output_filename = f\"TF_{key}.rds\"\n",
        "\n",
        "input_path  = os.path.join(os.getcwd(),  input_filename)\n",
        "output_path = os.path.join(os.getcwd(), output_filename)\n",
        "\n",
        "print(f\"Input data and model structure filename: {input_filename}\")\n",
        "print(f\"Output posteriors: {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6KVBalrZztk",
        "outputId": "866cb04b-6d5e-4585-f525-9bca3e7a03d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data and model structure filename: init_0ns_ns040_ny00100_chain01.rds\n",
            "Output posteriors: TF_0ns_ns040_ny00100.rds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "allas_bucket_path = \"https://a3s.fi/swift/v1/AUTH_3dd0cc28dd1a45d1a5e119173a48d4f5/2006339-big-spatial-init/\"\n",
        "\n",
        "try:\n",
        "    wget.download(os.path.join(allas_bucket_path, input_filename))\n",
        "except Exception as e:\n",
        "    print(f\"Could not download file {input_filename}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "UU9qKCCuMIj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Gibbs sampler"
      ],
      "metadata": {
        "id": "SOR4H60GfuYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAM = 25 # recorded samples from the posterior\n",
        "THIN = 10 # thinning between recorded samples"
      ],
      "metadata": {
        "id": "ePuqzn0WDQ5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python hmsc-hpc/hmsc/examples/run_gibbs_sampler.py \\\n",
        "\"--input\"=$input_path \\\n",
        "\"--output\"=$output_path \\\n",
        "\"--samples\"=$SAM \\\n",
        "\"--transient\"=${SAM*THIN} \\\n",
        "\"--thin\"=$THIN \\\n",
        "\"--verbose\"=100 \\\n",
        "\"--profile\"=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH-sAz9-aoc0",
        "outputId": "393a6437-9dba-48e3-c60b-7cd491f0eb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args=Namespace(samples=25, transient=50, thin=10, input='/content/init_0ns_ns040_ny00100_chain01.rds', output='/content/TF_0ns_ns040_ny00100.rds', verbose=100, tnlib='tf', fse=1, profile=1)\n",
            "/content\n",
            "Running TF Gibbs sampler:\n",
            "\n",
            "Initializing TF graph\n",
            "retracing\n",
            "Iterations 2\n",
            "\n",
            "Completed iterations 2\n",
            "\n",
            "\n",
            "Computing chain 0\n",
            "Iterations 300\n",
            "iteration 300 saving 25\n",
            "Completed iterations 300\n",
            "\n",
            "1 chains completed in 0.7 sec\n",
            "\n",
            "Whole fitting elapsed 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional. List available input data and model structures\n",
        "\n",
        "Note that the parameter grid contains all possible combinations, but some are missing from the data stored on Allas. These are typically the largest ones, and the reason is either that the initialization object was simply too large to store it with the currently used JSON+RDS approach, or that it is practically infeasible to compute the initialization object due to $n_y^3$ scaling (for GP models `init_1fu_*`)"
      ],
      "metadata": {
        "id": "jKLLe0V6gj32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "allas_bucket_path = 'https://a3s.fi/swift/v1/AUTH_3dd0cc28dd1a45d1a5e119173a48d4f5/2006339-big-spatial-init/'\n",
        "\n",
        "files = requests.get(allas_bucket_path).content.decode().split('\\n')\n",
        "for fileName in files:\n",
        "  print(fileName)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7fBBU13Qn__",
        "outputId": "0c6b90b3-72ef-41d0-f4d8-7c9851e51968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init_0ns_ns040_ny00100_chain01.rds\n",
            "init_0ns_ns040_ny00200_chain01.rds\n",
            "init_0ns_ns040_ny00400_chain01.rds\n",
            "init_0ns_ns040_ny00800_chain01.rds\n",
            "init_0ns_ns040_ny01600_chain01.rds\n",
            "init_0ns_ns040_ny03200_chain01.rds\n",
            "init_0ns_ns040_ny06400_chain01.rds\n",
            "init_0ns_ns040_ny103820_chain01.rds\n",
            "init_0ns_ns040_ny12800_chain01.rds\n",
            "init_0ns_ns040_ny207640_chain01.rds\n",
            "init_0ns_ns040_ny25955_chain01.rds\n",
            "init_0ns_ns040_ny51910_chain01.rds\n",
            "init_0ns_ns160_ny00100_chain01.rds\n",
            "init_0ns_ns160_ny00200_chain01.rds\n",
            "init_0ns_ns160_ny00400_chain01.rds\n",
            "init_0ns_ns160_ny00800_chain01.rds\n",
            "init_0ns_ns160_ny01600_chain01.rds\n",
            "init_0ns_ns160_ny03200_chain01.rds\n",
            "init_0ns_ns160_ny06400_chain01.rds\n",
            "init_0ns_ns160_ny103820_chain01.rds\n",
            "init_0ns_ns160_ny12800_chain01.rds\n",
            "init_0ns_ns160_ny207640_chain01.rds\n",
            "init_0ns_ns160_ny25955_chain01.rds\n",
            "init_0ns_ns160_ny51910_chain01.rds\n",
            "init_0ns_ns622_ny00100_chain01.rds\n",
            "init_0ns_ns622_ny00200_chain01.rds\n",
            "init_0ns_ns622_ny00400_chain01.rds\n",
            "init_0ns_ns622_ny00800_chain01.rds\n",
            "init_0ns_ns622_ny01600_chain01.rds\n",
            "init_0ns_ns622_ny03200_chain01.rds\n",
            "init_0ns_ns622_ny06400_chain01.rds\n",
            "init_0ns_ns622_ny103820_chain01.rds\n",
            "init_0ns_ns622_ny12800_chain01.rds\n",
            "init_0ns_ns622_ny25955_chain01.rds\n",
            "init_0ns_ns622_ny51910_chain01.rds\n",
            "init_1fu_ns040_ny00100_chain01.rds\n",
            "init_1fu_ns040_ny00200_chain01.rds\n",
            "init_1fu_ns040_ny00400_chain01.rds\n",
            "init_1fu_ns040_ny00800_chain01.rds\n",
            "init_1fu_ns040_ny01600_chain01.rds\n",
            "init_1fu_ns040_ny03200_chain01.rds\n",
            "init_1fu_ns160_ny00100_chain01.rds\n",
            "init_1fu_ns160_ny00200_chain01.rds\n",
            "init_1fu_ns160_ny00400_chain01.rds\n",
            "init_1fu_ns160_ny00800_chain01.rds\n",
            "init_1fu_ns160_ny01600_chain01.rds\n",
            "init_1fu_ns160_ny03200_chain01.rds\n",
            "init_1fu_ns622_ny00100_chain01.rds\n",
            "init_1fu_ns622_ny00200_chain01.rds\n",
            "init_1fu_ns622_ny00400_chain01.rds\n",
            "init_1fu_ns622_ny00800_chain01.rds\n",
            "init_1fu_ns622_ny01600_chain01.rds\n",
            "init_1fu_ns622_ny03200_chain01.rds\n",
            "init_2pg_ns040_ny00100_chain01.rds\n",
            "init_2pg_ns040_ny00200_chain01.rds\n",
            "init_2pg_ns040_ny00400_chain01.rds\n",
            "init_2pg_ns040_ny00800_chain01.rds\n",
            "init_2pg_ns040_ny01600_chain01.rds\n",
            "init_2pg_ns040_ny03200_chain01.rds\n",
            "init_2pg_ns040_ny06400_chain01.rds\n",
            "init_2pg_ns040_ny103820_chain01.rds\n",
            "init_2pg_ns040_ny12800_chain01.rds\n",
            "init_2pg_ns040_ny25955_chain01.rds\n",
            "init_2pg_ns040_ny51910_chain01.rds\n",
            "init_2pg_ns160_ny00100_chain01.rds\n",
            "init_2pg_ns160_ny00200_chain01.rds\n",
            "init_2pg_ns160_ny00400_chain01.rds\n",
            "init_2pg_ns160_ny00800_chain01.rds\n",
            "init_2pg_ns160_ny01600_chain01.rds\n",
            "init_2pg_ns160_ny03200_chain01.rds\n",
            "init_2pg_ns160_ny06400_chain01.rds\n",
            "init_2pg_ns160_ny103820_chain01.rds\n",
            "init_2pg_ns160_ny12800_chain01.rds\n",
            "init_2pg_ns160_ny25955_chain01.rds\n",
            "init_2pg_ns160_ny51910_chain01.rds\n",
            "init_2pg_ns622_ny00100_chain01.rds\n",
            "init_2pg_ns622_ny00200_chain01.rds\n",
            "init_2pg_ns622_ny00400_chain01.rds\n",
            "init_2pg_ns622_ny00800_chain01.rds\n",
            "init_2pg_ns622_ny01600_chain01.rds\n",
            "init_2pg_ns622_ny03200_chain01.rds\n",
            "init_2pg_ns622_ny06400_chain01.rds\n",
            "init_2pg_ns622_ny103820_chain01.rds\n",
            "init_2pg_ns622_ny12800_chain01.rds\n",
            "init_2pg_ns622_ny25955_chain01.rds\n",
            "init_2pg_ns622_ny51910_chain01.rds\n"
          ]
        }
      ]
    }
  ]
}